<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Classification Error Metrics - Training Presentation</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }

        .header {
            text-align: center;
            background: rgba(255, 255, 255, 0.95);
            padding: 40px 20px;
            border-radius: 15px;
            margin-bottom: 30px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }

        .header h1 {
            font-size: 2.5em;
            color: #2c3e50;
            margin-bottom: 10px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .header p {
            font-size: 1.2em;
            color: #7f8c8d;
        }

        .section {
            background: rgba(255, 255, 255, 0.95);
            margin-bottom: 25px;
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
            transition: transform 0.3s ease;
        }

        .section:hover {
            transform: translateY(-5px);
        }

        .section h2 {
            color: #2c3e50;
            font-size: 1.8em;
            margin-bottom: 20px;
            border-bottom: 3px solid #667eea;
            padding-bottom: 10px;
        }

        .section h3 {
            color: #34495e;
            font-size: 1.4em;
            margin: 25px 0 15px 0;
        }

        .formula {
            background: #f8f9fa;
            border-left: 4px solid #667eea;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            font-size: 1.1em;
            border-radius: 5px;
        }

        .example-box {
            background: linear-gradient(135deg, #e3f2fd, #bbdefb);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            border-left: 5px solid #2196f3;
        }

        .warning-box {
            background: linear-gradient(135deg, #fff3e0, #ffe0b2);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            border-left: 5px solid #ff9800;
        }

        .key-points {
            background: linear-gradient(135deg, #e8f5e8, #c8e6c9);
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            border-left: 5px solid #4caf50;
        }

        .confusion-matrix {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 10px;
            margin: 20px 0;
            max-width: 400px;
        }

        .matrix-cell {
            background: #f8f9fa;
            border: 2px solid #dee2e6;
            padding: 15px;
            text-align: center;
            border-radius: 8px;
            font-weight: bold;
        }

        .matrix-header {
            background: #667eea;
            color: white;
        }

        .matrix-tp { background: #d4edda; }
        .matrix-fp { background: #f8d7da; }
        .matrix-fn { background: #f8d7da; }
        .matrix-tn { background: #d4edda; }

        ul {
            padding-left: 25px;
            margin: 15px 0;
        }

        li {
            margin-bottom: 8px;
        }

        .metric-comparison {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }

        .metric-card {
            background: linear-gradient(135deg, #f5f7fa, #c3cfe2);
            padding: 20px;
            border-radius: 10px;
            text-align: center;
            border: 2px solid #e0e6ed;
        }

        .metric-card h4 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 1.3em;
        }

        @media print {
            body {
                background: white;
            }
            .section {
                break-inside: avoid;
                page-break-inside: avoid;
            }
        }

        @media (max-width: 768px) {
            .header h1 {
                font-size: 2em;
            }
            .section {
                padding: 20px;
            }
            .confusion-matrix {
                font-size: 0.9em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Classification Error Metrics</h1>
            <p>Evaluating Performance for Machine Learning Classification Problems</p>
        </div>

        <div class="section">
            <h2>üéØ Learning Objectives</h2>
            <div class="key-points">
                <p><strong>By the end of this training, you will understand:</strong></p>
                <ul>
                    <li>Core classification metrics: Accuracy, Precision, Recall, and F1-Score</li>
                    <li>How to interpret and calculate these metrics</li>
                    <li>When and why different metrics matter</li>
                    <li>The importance of context in model evaluation</li>
                    <li>How to use confusion matrices effectively</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>üìä The Foundation: Correct vs. Incorrect Predictions</h2>
            <p>All classification metrics stem from one fundamental concept: <strong>a model's prediction is either correct or incorrect</strong>.</p>
            
            <div class="example-box">
                <h3>üêï Example: Dog vs. Cat Classification</h3>
                <p>Imagine we have a model that classifies images as either "dog" or "cat":</p>
                <ul>
                    <li>We feed test images to our trained model</li>
                    <li>Model outputs a prediction (dog or cat)</li>
                    <li>We compare the prediction to the true label</li>
                    <li>Result: Either correct or incorrect</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>üéØ Accuracy: The Most Intuitive Metric</h2>
            
            <div class="formula">
                <strong>Accuracy = Number of Correct Predictions / Total Number of Predictions</strong>
            </div>

            <div class="example-box">
                <p><strong>Example:</strong> If we test 100 images and our model correctly predicts 80 of them:</p>
                <p><strong>Accuracy = 80/100 = 80%</strong></p>
            </div>

            <h3>‚ö†Ô∏è When Accuracy Can Be Misleading</h3>
            <div class="warning-box">
                <p><strong>Imbalanced Dataset Problem:</strong></p>
                <p>Test set: 99 dog images, 1 cat image</p>
                <p>Model that always predicts "dog" = 99% accuracy!</p>
                <p><strong>But it never correctly identifies cats!</strong></p>
            </div>

            <div class="key-points">
                <p><strong>Key Takeaway:</strong> Accuracy works well for balanced datasets but can be misleading with imbalanced classes.</p>
            </div>
        </div>

        <div class="section">
            <h2>üîç Understanding the Confusion Matrix</h2>
            <p>A confusion matrix organizes classification results by comparing predicted vs. true conditions:</p>

            <div class="confusion-matrix">
                <div class="matrix-cell matrix-header"></div>
                <div class="matrix-cell matrix-header">Predicted Positive</div>
                <div class="matrix-cell matrix-header">Predicted Negative</div>
                
                <div class="matrix-cell matrix-header">Actual Positive</div>
                <div class="matrix-cell matrix-tp">True Positive (TP)</div>
                <div class="matrix-cell matrix-fn">False Negative (FN)</div>
                
                <div class="matrix-cell matrix-header">Actual Negative</div>
                <div class="matrix-cell matrix-fp">False Positive (FP)</div>
                <div class="matrix-cell matrix-tn">True Negative (TN)</div>
            </div>

            <div class="metric-comparison">
                <div class="metric-card">
                    <h4>True Positive (TP)</h4>
                    <p>Correctly predicted positive cases</p>
                </div>
                <div class="metric-card">
                    <h4>True Negative (TN)</h4>
                    <p>Correctly predicted negative cases</p>
                </div>
                <div class="metric-card">
                    <h4>False Positive (FP)</h4>
                    <p>Incorrectly predicted positive (Type I Error)</p>
                </div>
                <div class="metric-card">
                    <h4>False Negative (FN)</h4>
                    <p>Incorrectly predicted negative (Type II Error)</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>üéØ Precision: Quality of Positive Predictions</h2>
            
            <div class="formula">
                <strong>Precision = True Positives / (True Positives + False Positives)</strong>
            </div>

            <p><strong>Precision answers:</strong> "Of all the cases I predicted as positive, how many were actually positive?"</p>

            <div class="example-box">
                <p><strong>Medical Example:</strong> Of all patients your model diagnosed with disease, what percentage actually had the disease?</p>
                <p><strong>High Precision = Low False Alarms</strong></p>
            </div>
        </div>

        <div class="section">
            <h2>üîç Recall: Completeness of Positive Detection</h2>
            
            <div class="formula">
                <strong>Recall = True Positives / (True Positives + False Negatives)</strong>
            </div>

            <p><strong>Recall answers:</strong> "Of all actual positive cases, how many did I correctly identify?"</p>

            <div class="example-box">
                <p><strong>Medical Example:</strong> Of all patients who actually had the disease, what percentage did your model correctly identify?</p>
                <p><strong>High Recall = Few Missed Cases</strong></p>
            </div>
        </div>

        <div class="section">
            <h2>‚öñÔ∏è The Precision-Recall Trade-off</h2>
            
            <div class="warning-box">
                <p><strong>You can't always have both high precision AND high recall!</strong></p>
                <ul>
                    <li><strong>Increase precision:</strong> Be more conservative ‚Üí fewer false positives, but might miss some positives (lower recall)</li>
                    <li><strong>Increase recall:</strong> Be more liberal ‚Üí catch more positives, but might include some false positives (lower precision)</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>üéØ F1-Score: The Best of Both Worlds</h2>
            
            <div class="formula">
                <strong>F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)</strong>
            </div>

            <p>The F1-score is the <strong>harmonic mean</strong> of precision and recall.</p>

            <div class="key-points">
                <p><strong>Why harmonic mean?</strong></p>
                <ul>
                    <li>Punishes extreme values</li>
                    <li>Perfect precision (1.0) + zero recall (0.0) = F1-score of 0</li>
                    <li>Provides balanced assessment of model performance</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>üìä Calculating Metrics from Confusion Matrix</h2>
            
            <div class="formula">
                <strong>Accuracy = (TP + TN) / (TP + TN + FP + FN)</strong><br><br>
                <strong>Precision = TP / (TP + FP)</strong><br><br>
                <strong>Recall = TP / (TP + FN)</strong><br><br>
                <strong>F1-Score = 2 √ó (Precision √ó Recall) / (Precision + Recall)</strong>
            </div>
        </div>

        <div class="section">
            <h2>üè• Real-World Context: Medical Diagnosis</h2>
            
            <div class="example-box">
                <h3>Disease Screening Scenario</h3>
                <p>A urine test for prostate cancer before a biopsy:</p>
                <ul>
                    <li><strong>False Negative:</strong> Miss a cancer case ‚Üí Patient doesn't get biopsy ‚Üí Disease goes untreated</li>
                    <li><strong>False Positive:</strong> Healthy patient gets unnecessary biopsy ‚Üí Inconvenient but not life-threatening</li>
                </ul>
            </div>

            <div class="key-points">
                <p><strong>Priority:</strong> Minimize False Negatives (maximize Recall)</p>
                <p><strong>Reasoning:</strong> Better to have some false alarms than to miss actual disease cases</p>
            </div>
        </div>

        <div class="section">
            <h2>ü§ù Working with Domain Experts</h2>
            
            <div class="warning-box">
                <p><strong>Important:</strong> There are NO universal "good enough" thresholds for metrics!</p>
            </div>

            <div class="key-points">
                <p><strong>Always collaborate with domain experts to determine:</strong></p>
                <ul>
                    <li>Which metric is most important for your specific use case</li>
                    <li>Acceptable levels of false positives vs. false negatives</li>
                    <li>Business or clinical consequences of different types of errors</li>
                    <li>How the model fits into the larger workflow</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>üìù Summary & Key Takeaways</h2>
            
            <div class="metric-comparison">
                <div class="metric-card">
                    <h4>Accuracy</h4>
                    <p>Overall correctness. Good for balanced datasets.</p>
                </div>
                <div class="metric-card">
                    <h4>Precision</h4>
                    <p>Quality of positive predictions. Important when false positives are costly.</p>
                </div>
                <div class="metric-card">
                    <h4>Recall</h4>
                    <p>Completeness of positive detection. Critical when false negatives are dangerous.</p>
                </div>
                <div class="metric-card">
                    <h4>F1-Score</h4>
                    <p>Balanced measure when you need both precision and recall.</p>
                </div>
            </div>

            <div class="key-points">
                <p><strong>Remember:</strong></p>
                <ul>
                    <li>No single metric tells the complete story</li>
                    <li>Context and domain expertise are essential</li>
                    <li>Consider the real-world consequences of different types of errors</li>
                    <li>Use confusion matrices to understand your model's behavior</li>
                    <li>Collaborate with stakeholders to define success metrics</li>
                </ul>
            </div>
        </div>

        <div class="section">
            <h2>‚ùì Questions for Discussion</h2>
            <ul>
                <li>In your domain, which type of error (false positive vs. false negative) would be more problematic?</li>
                <li>How would you explain these metrics to a non-technical stakeholder?</li>
                <li>Can you think of scenarios where accuracy might be misleading in your work?</li>
                <li>What additional context would you need from domain experts for your projects?</li>
            </ul>
        </div>
    </div>
</body>
</html>
